{"paragraphs":[{"title":"How to register user define function","text":"%pyspark\nfrom pyspark.sql.types import *\nsqlContext.udf.register(\"run_cmd\", run_cmd, ArrayType(StringType()))","user":"anonymous","dateUpdated":"2020-04-01T06:39:18+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585723158133_1059175446","id":"20200330-131418_702619655","dateCreated":"2020-04-01T06:39:18+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11326"},{"title":"Functions","text":"%pyspark\nimport zipfile\nimport os.path\nimport subprocess\nfrom pyspark.ml.fpm import FPGrowth\nfrom pyspark.sql.functions import col, size\nfrom os import path\n\ndef run_cmd(args_list):\n  \"\"\"\n  run linux commands\n  \"\"\"\n  # import subprocess\n  print('Running system command: {0}'.format(' '.join(args_list)))\n  proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  s_output, s_err = proc.communicate()\n  s_return =  proc.returncode\n  return s_return, s_output, s_err\ndef uploadFile(url):\n  filename = url.split(\"/\")[-1]\n  csvfile=\"\"\n  print(\"filename:\"+filename)\n  if path.exists(filename):\n    print(\"file exist\")\n    zip = zipfile.ZipFile(filename)\n    csvfile=zip.namelist()[0]\n  else:\n    data=['wget',url]\n    print(data);\n    run_cmd(data)\n    zip = zipfile.ZipFile(filename)\n    csvfile=zip.namelist()[0]\n    print(\"csvfile:\"+csvfile)\n    run_cmd(['unzip',filename])\n    run_cmd(['hdfs','dfs','-mkdir','-p','/user/zeppelin/kmutt'])\n    run_cmd(['hdfs','dfs','-put',csvfile,'/user/zeppelin/kmutt'])\n    \n  print(\"----------------\")  \n  csvfile=\"/user/zeppelin/kmutt/\"+csvfile\n  #raw_data=pd.read_csv(csvfile)\n  print(\"csvfile:\"+csvfile)\n  raw_data=spark.read.format(\"csv\").option(\"header\", \"true\").load(csvfile)#spark.read.csv(csvfile,inferSchema=True,header=True)\n  return raw_data\ndef test(dataframe,s):\n  return dataframe;\ndef test2(dataframe,s):\n  return dataframe\n#create a function\ndef mean_age_by_group(dataframe,col):\n  #print(\"count:\"+dataframe.count())\n  return dataframe\ndef cleansing(df):\n    mySchema = StructType([ StructField(\"Invoice\", IntegerType(), True)\\\n                       ,StructField(\"StockCode\", StringType(), True)\\\n                       ,StructField(\"Description\", StringType(), True)\\\n                       ,StructField(\"Quantity\", StringType(), True)\\\n                       ,StructField(\"InvoiceDate\", StringType(), True)\\\n                       ,StructField(\"Customer ID\", StringType(), True)\\\n                       ,StructField(\"InvoiceDate\", StringType(), True)\\\n                       ,StructField(\"Price\", StringType(), True)\n                       ])\n    #df_spark = spark.createDataFrame(dataframe,schema=mySchema)\n    #print(\"data before cleansing:\"+df_spark.count()+ \"instances\")\n    #df_spark=df_spark.filter(col(\"Length\")===6)\n    #df_spark.filter(col(\"Country\") =!= \"United Kingdom\")#.filter(col(\"Length\")===6).filter(col(\"Description\").isNotNull)#.selectExpr(\"cast(Invoice as int) Invoice\", \"StockCode\").write.mode(\"overwrite\").saveAsTable(\"marketbasket.Stocks\")\n    #return df_spark.toPandas()\n    #dataframe=dataframe.filter(col(\"Country\") != \"United Kingdom\")\n    #\n    #convert String to Integer because Invoice is not String\n    df.dropna(inplace=True, subset=['Invoice'])\n    to_drop =df[df.Invoice.str.match('^[a-zA-Z]')].index\n    df.drop(to_drop, axis=0, inplace=True)\n    df.Invoice = df.Invoice.astype('int64')\n    df.dropna(inplace=True, subset=['Description'])\n    df.dropna(inplace=True, subset=['StockCode'])\n    return df\ndef countrySelection(df,str):\n    if str<>\"\":\n        df=df[df.Country==str]\n    return df\ndef delCountrySelection(df,str):\n    df=df[df.Country!=str]\n    return df\ndef createDatabaseOnHive(df):\n    spark.sql('CREATE DATABASE IF NOT EXISTS marketbasket')\n    return df\ndef delStocksTableOnHive(df):\n    spark.sql('DROP TABLE IF EXISTS marketbasket.Stocks') \n    return df\ndef delItemsTableOnHive(df):\n    spark.sql('DROP TABLE IF EXISTS marketbasket.items') \n    return df\ndef loadData2Stock(df_panda):\n    spark_df = spark.createDataFrame(df_panda)\n    spark_df.selectExpr(\"Invoice\", \"StockCode\").write.mode(\"overwrite\").saveAsTable(\"marketbasket.Stocks\")\n    return df_panda\ndef loadData2Items(df_panda):\n    spark_df = spark.createDataFrame(df_panda)\n    spark_df.groupBy(\"StockCode\",\"Description\").count().write.mode(\"overwrite\").saveAsTable(\"marketbasket.Items\")\n    return df_panda\ndef transfromData():\n    transConvert=spark.sql(\"SELECT DISTINCT invoice,stockcode FROM marketbasket.Stocks\").rdd.map(lambda row:(row[0],[row[1]])).reduceByKey(lambda x,y:x+y).toDF([\"invoiceid\",\"items\"])\n    #transConvert.show(5,truncate=False)\n    #z.put(\"transConvert\", transConvert)\n    return transConvert\ndef train2Model(transConvert,minSup,minConfi,invoiceNums):\n    fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=minSup/(invoiceNums+.01), minConfidence=minConfi)\n    model_market = fpGrowth.fit(transConvert)\n    return model_market\ndef useModel(transConvert,model_market):\n    model_market.transform(transConvert).where(size(col(\"prediction\"))>0)\n    return model_market\ndef checkRole(model_market):\n    associationRules=model_market.associationRules.sort('confidence',ascending=False)\n    #associationRules.show(10)\n    #z.show(associationRules)\n    return associationRules\ndef showItemsetModel():\n    model_market.freqItemsets.show()\ndef showProductInRole(associationRules):\n    prodId=associationRules.rdd.flatMap(lambda x:x.antecedent+x.consequent).distinct().collect()\n    prodId=[[i] for i in prodId]\n    prodInRules=sc.parallelize(prodId).toDF(['prod_id'])\n    prodInRules.registerTempTable(\"prodInRules\")\n    freqProduct=spark.sql(\"SELECT stockcode, description FROM prodInRules JOIN marketbasket.items ON prodInRules.prod_id=items.stockcode\");\n    return freqProduct","user":"anonymous","dateUpdated":"2020-04-01T14:15:07+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585723158141_1994077935","id":"20200330-182223_513629265","dateCreated":"2020-04-01T06:39:18+0000","dateStarted":"2020-04-01T14:15:07+0000","dateFinished":"2020-04-01T14:15:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11327"},{"title":"upload file ,cleansing data and dataframe to hive by pipeline","text":"%pyspark\nimport pandas as pd\nfrom pyspark.sql.types import *\ndf=uploadFile(\"https://raw.githubusercontent.com/ekachaiou/KMUTT-PROJ/master/Year2010-2011.zip\")\ndf_bd=df.toPandas()\ndf1=(df_bd.pipe(cleansing)\n   .pipe(countrySelection,str='France')\n   .pipe(createDatabaseOnHive)\n   .pipe(delStocksTableOnHive)\n   .pipe(loadData2Stock)\n   .pipe(delItemsTableOnHive)\n   .pipe(loadData2Items)\n   )","user":"anonymous","dateUpdated":"2020-04-01T14:15:36+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filename:Year2010-2011.zip\nfile exist\n----------------\ncsvfile:/user/zeppelin/kmutt/Year2010-2011.csv\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://cluster-ou-m.c.verdant-cable-259303.internal:4040/jobs/job?id=147","http://cluster-ou-m.c.verdant-cable-259303.internal:4040/jobs/job?id=148","http://cluster-ou-m.c.verdant-cable-259303.internal:4040/jobs/job?id=149","http://cluster-ou-m.c.verdant-cable-259303.internal:4040/jobs/job?id=150"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1585723158142_-651094787","id":"20200330-174031_1657390125","dateCreated":"2020-04-01T06:39:18+0000","dateStarted":"2020-04-01T14:15:36+0000","dateFinished":"2020-04-01T14:16:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11328"}],"name":"BigCer/Market-Basket-Analysis-cleansing&data2hive","id":"2F43RJ6EJ","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}