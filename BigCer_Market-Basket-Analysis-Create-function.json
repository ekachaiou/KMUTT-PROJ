{"paragraphs":[{"title":"How to register user define function","text":"%pyspark\nfrom pyspark.sql.types import *\nsqlContext.udf.register(\"run_cmd\", run_cmd, ArrayType(StringType()))","user":"anonymous","dateUpdated":"2020-03-31T06:54:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585574058809_-1784683559","id":"20200330-131418_702619655","dateCreated":"2020-03-30T13:14:18+0000","dateStarted":"2020-03-30T18:18:06+0000","dateFinished":"2020-03-30T18:18:06+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:751"},{"title":"Functions","text":"%pyspark\nimport zipfile\nimport os.path\nimport subprocess\nfrom pyspark.ml.fpm import FPGrowth\nfrom pyspark.sql.functions import col, size\nfrom os import path\n\ndef run_cmd(args_list):\n  \"\"\"\n  run linux commands\n  \"\"\"\n  # import subprocess\n  print('Running system command: {0}'.format(' '.join(args_list)))\n  proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  s_output, s_err = proc.communicate()\n  s_return =  proc.returncode\n  return s_return, s_output, s_err\ndef uploadFile(url):\n  filename = url.split(\"/\")[-1]\n  csvfile=\"\"\n  print(\"filename:\"+filename)\n  if path.exists(filename):\n    print(\"file exist\")\n    zip = zipfile.ZipFile(filename)\n    csvfile=zip.namelist()[0]\n  else:\n    data=['wget',url]\n    print(data);\n    run_cmd(data)\n    zip = zipfile.ZipFile(filename)\n    csvfile=zip.namelist()[0]\n    print(\"csvfile:\"+csvfile)\n    run_cmd(['unzip',filename])\n    run_cmd(['hdfs','dfs','-mkdir','-p','/user/zeppelin/kmutt'])\n    run_cmd(['hdfs','dfs','-put',csvfile,'/user/zeppelin/kmutt'])\n    \n  print(\"----------------\")  \n  csvfile=\"/user/zeppelin/kmutt/\"+csvfile\n  #raw_data=pd.read_csv(csvfile)\n  print(\"csvfile:\"+csvfile)\n  raw_data=spark.read.format(\"csv\").option(\"header\", \"true\").load(csvfile)#spark.read.csv(csvfile,inferSchema=True,header=True)\n  return raw_data\ndef test(dataframe,s):\n  return dataframe;\ndef test2(dataframe,s):\n  return dataframe\n#create a function\ndef mean_age_by_group(dataframe,col):\n  #print(\"count:\"+dataframe.count())\n  return dataframe\ndef cleansing(df):\n    mySchema = StructType([ StructField(\"Invoice\", IntegerType(), True)\\\n                       ,StructField(\"StockCode\", StringType(), True)\\\n                       ,StructField(\"Description\", StringType(), True)\\\n                       ,StructField(\"Quantity\", StringType(), True)\\\n                       ,StructField(\"InvoiceDate\", StringType(), True)\\\n                       ,StructField(\"Customer ID\", StringType(), True)\\\n                       ,StructField(\"InvoiceDate\", StringType(), True)\\\n                       ,StructField(\"Price\", StringType(), True)\n                       ])\n    #df_spark = spark.createDataFrame(dataframe,schema=mySchema)\n    #print(\"data before cleansing:\"+df_spark.count()+ \"instances\")\n    #df_spark=df_spark.filter(col(\"Length\")===6)\n    #df_spark.filter(col(\"Country\") =!= \"United Kingdom\")#.filter(col(\"Length\")===6).filter(col(\"Description\").isNotNull)#.selectExpr(\"cast(Invoice as int) Invoice\", \"StockCode\").write.mode(\"overwrite\").saveAsTable(\"marketbasket.Stocks\")\n    #return df_spark.toPandas()\n    #dataframe=dataframe.filter(col(\"Country\") != \"United Kingdom\")\n    #\n    #convert String to Integer because Invoice is not String\n    df.dropna(inplace=True, subset=['Invoice'])\n    to_drop =df[df.Invoice.str.match('^[a-zA-Z]')].index\n    df.drop(to_drop, axis=0, inplace=True)\n    df.Invoice = df.Invoice.astype('int64')\n    df.dropna(inplace=True, subset=['Description'])\n    df.dropna(inplace=True, subset=['StockCode'])\n    return df\ndef countrySelection(df,str):\n    df=df[df.Country==str]\n    return df\ndef delCountrySelection(df,str):\n    df=df[df.Country!=str]\n    return df\ndef createDatabaseOnHive(df):\n    spark.sql('CREATE DATABASE IF NOT EXISTS marketbasket')\n    return df\ndef delStocksTableOnHive(df):\n    spark.sql('DROP TABLE IF EXISTS marketbasket.Stocks') \n    return df\ndef delItemsTableOnHive(df):\n    spark.sql('DROP TABLE IF EXISTS marketbasket.items') \n    return df\ndef loadData2Stock(df_panda):\n    spark_df = spark.createDataFrame(df_panda)\n    spark_df.selectExpr(\"Invoice\", \"StockCode\").write.mode(\"overwrite\").saveAsTable(\"marketbasket.Stocks\")\n    return df_panda\ndef loadData2Items(df_panda):\n    spark_df = spark.createDataFrame(df_panda)\n    spark_df.groupBy(\"StockCode\",\"Description\").count().write.mode(\"overwrite\").saveAsTable(\"marketbasket.Items\")\n    return df_panda\ndef transfromData():\n    transConvert=spark.sql(\"SELECT DISTINCT invoice,stockcode FROM marketbasket.Stocks\").rdd.map(lambda row:(row[0],[row[1]])).reduceByKey(lambda x,y:x+y).toDF([\"invoiceid\",\"items\"])\n    #transConvert.show(5,truncate=False)\n    #z.put(\"transConvert\", transConvert)\n    return transConvert\ndef train2Model(transConvert,minSup,minConfi,invoiceNums):\n    fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=minSup/(invoiceNums+.01), minConfidence=minConfi)\n    model_market = fpGrowth.fit(transConvert)\n    return model_market\ndef useModel(transConvert,model_market):\n    model_market.transform(transConvert).where(size(col(\"prediction\"))>0)\n    return model_market\ndef checkRole(model_market):\n    associationRules=model_market.associationRules.sort('confidence',ascending=False)\n    #associationRules.show(10)\n    #z.show(associationRules)\n    return associationRules\ndef showItemsetModel():\n    model_market.freqItemsets.show()\ndef showProductInRole(associationRules):\n    prodId=associationRules.rdd.flatMap(lambda x:x.antecedent+x.consequent).distinct().collect()\n    prodId=[[i] for i in prodId]\n    prodInRules=sc.parallelize(prodId).toDF(['prod_id'])\n    prodInRules.registerTempTable(\"prodInRules\")\n    freqProduct=spark.sql(\"SELECT stockcode, description FROM prodInRules JOIN marketbasket.items ON prodInRules.prod_id=items.stockcode\");\n    return freqProduct","user":"anonymous","dateUpdated":"2020-03-31T19:36:17+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585592543824_455760837","id":"20200330-182223_513629265","dateCreated":"2020-03-30T18:22:23+0000","dateStarted":"2020-03-31T19:36:17+0000","dateFinished":"2020-03-31T19:36:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:752"},{"title":"upload file ,cleansing data and dataframe to hive by pipeline","text":"%pyspark\nimport pandas as pd\nfrom pyspark.sql.types import *\ndf=uploadFile(\"https://raw.githubusercontent.com/ekachaiou/KMUTT-PROJ/master/Year2010-2011.zip\")\ndf_bd=df.toPandas()\ndf1=(df_bd.pipe(cleansing)\n   .pipe(countrySelection,str='France')\n   .pipe(createDatabaseOnHive)\n   .pipe(delStocksTableOnHive)\n   .pipe(loadData2Stock)\n   .pipe(delStocksTableOnHive)\n   .pipe(loadData2Stock)\n   )","user":"anonymous","dateUpdated":"2020-03-31T19:36:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filename:Year2010-2011.zip\nfile exist\n----------------\ncsvfile:/user/zeppelin/kmutt/Year2010-2011.csv\n"}]},"apps":[],"jobName":"paragraph_1585590031829_-1179470544","id":"20200330-174031_1657390125","dateCreated":"2020-03-30T17:40:31+0000","dateStarted":"2020-03-31T19:36:59+0000","dateFinished":"2020-03-31T19:37:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:753"},{"title":"transform data to assoication algorithm","text":"%pyspark\ntransData=transfromData()\n","user":"anonymous","dateUpdated":"2020-03-31T19:37:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585636271480_856045542","id":"20200331-063111_1271081525","dateCreated":"2020-03-31T06:31:11+0000","dateStarted":"2020-03-31T19:37:54+0000","dateFinished":"2020-03-31T19:38:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:754"},{"text":"%pyspark\ntransData.show()","user":"anonymous","dateUpdated":"2020-03-31T19:38:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+--------------------+\n|invoiceid|               items|\n+---------+--------------------+\n|   544200|[21843, 84997C, 2...|\n|   565801|[23209, 22663, 22...|\n|   581001|[22326, 22540, 23...|\n|   563202|      [22797, 85066]|\n|   573402|[23307, POST, 233...|\n|   569402|[22457, 22138, 85...|\n|   541405|[21058, 20966, 21...|\n|   548606|[22398, 10002, 22...|\n|   562207|[21213, 22085, 21...|\n|   539607|[21865, 22556, PO...|\n|   539407|[22492, POST, 222...|\n|   559607|[22467, 22326, 23...|\n|   538008|[21094, 21154, PO...|\n|   553208|[22693, 22492, PO...|\n|   570409|[23203, 21212, 23...|\n|   548409|[22549, 22630, 48...|\n|   548410|[23010, 23007, 22...|\n|   553411|[23049, 23191, 23...|\n|   565612|[22551, 22635, 21...|\n|   558813|[22556, 23084, PO...|\n+---------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1585641810636_517423455","id":"20200331-080330_1850085137","dateCreated":"2020-03-31T08:03:30+0000","dateStarted":"2020-03-31T19:38:44+0000","dateFinished":"2020-03-31T19:38:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:755"},{"title":"train of model","text":"%pyspark\nmodel=train2Model(transData,10,0.9,df1.Invoice.count())","user":"anonymous","dateUpdated":"2020-03-31T19:38:57+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1585638114373_-1652873114","id":"20200331-070154_201418204","dateCreated":"2020-03-31T07:01:54+0000","dateStarted":"2020-03-31T19:38:57+0000","dateFinished":"2020-03-31T19:39:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:756"},{"text":"%pyspark\nassociationRules=model.associationRules.sort('confidence',ascending=False)\nassociationRules.show(10)\nz.show(associationRules)","user":"anonymous","dateUpdated":"2020-03-31T23:18:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 2: associationRules.show(10)\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7418825847220642752.py\", line 375, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 350, in show\n    print(self._jdf.showString(n, 20, vertical))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 67, in deco\n    e.java_exception.getStackTrace()))\n  File \"/usr/lib/python2.7/_abcoll.py\", line 605, in __iter__\n    v = self[i]\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\", line 191, in __getitem__\n    return self.__compute_item(key)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\", line 174, in __compute_item\n    return get_return_value(answer, self._gateway_client)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 342, in get_return_value\n    return OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2463, in <lambda>\n    lambda target_id, gateway_client: JavaObject(target_id, gateway_client))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1295, in __init__\n    ThreadSafeFinalizer.add_finalizer(key, value)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/finalizer.py\", line 43, in add_finalizer\n    cls.finalizers[id] = weak_ref\n  File \"/usr/lib/python2.7/threading.py\", line 215, in __exit__\n    def __exit__(self, t, v, tb):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 266, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"}]},"apps":[],"jobName":"paragraph_1585696621777_212528266","id":"20200331-231701_268313953","dateCreated":"2020-03-31T23:17:01+0000","dateStarted":"2020-03-31T23:18:11+0000","dateFinished":"2020-03-31T23:25:19+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:757"},{"text":"%pyspark\nassociationRules=checkRole(model)\nz.show(associationRules)","user":"anonymous","dateUpdated":"2020-03-31T23:12:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 2: z.show(associationRules)\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7418825847220642752.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"/tmp/zeppelin_pyspark-7418825847220642752.py\", line 52, in show\n    print(self.z.showData(obj._jdf))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n    answer = self.gateway_client.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 991, in send_command\n    self._give_back_connection(connection)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 942, in _give_back_connection\n    self.deque.append(connection)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 266, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"}]},"apps":[],"jobName":"paragraph_1585641556932_242656448","id":"20200331-075916_918646502","dateCreated":"2020-03-31T07:59:16+0000","dateStarted":"2020-03-31T23:12:50+0000","dateFinished":"2020-03-31T23:15:39+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:758"},{"title":"productlist for predicting","text":"%pyspark\nfreqProduct=showProductInRole(associationRules)\nfreqProduct.show(truncate=False)","user":"anonymous","dateUpdated":"2020-03-31T20:17:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585636846193_573732861","id":"20200331-064046_1864680092","dateCreated":"2020-03-31T06:40:46+0000","dateStarted":"2020-03-31T20:17:02+0000","dateFinished":"2020-03-31T20:17:13+0000","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:759"},{"text":"%pyspark\ndf1.count()","user":"anonymous","dateUpdated":"2020-03-31T23:15:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Invoice        8409\nStockCode      8409\nDescription    8409\nQuantity       8409\nInvoiceDate    8409\nPrice          8409\nCustomer ID    8343\nCountry        8409\ndtype: int64\n"}]},"apps":[],"jobName":"paragraph_1585637553714_1493347398","id":"20200331-065233_1413730047","dateCreated":"2020-03-31T06:52:33+0000","dateStarted":"2020-03-31T23:15:51+0000","dateFinished":"2020-03-31T23:15:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:760"},{"text":"%pyspark\ntransData.show()\n","user":"anonymous","dateUpdated":"2020-03-31T23:25:31+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 1: transData.show()\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7418825847220642752.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 350, in show\n    print(self._jdf.showString(n, 20, vertical))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 67, in deco\n    e.java_exception.getStackTrace()))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 66, in <lambda>\n    stackTrace = '\\n\\t at '.join(map(lambda x: x.toString(),\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n    answer = self.gateway_client.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n    response = connection.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1152, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n  File \"/usr/lib/python2.7/socket.py\", line 466, in readline\n    return buf.getvalue()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 266, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"}]},"apps":[],"jobName":"paragraph_1585640692539_358551412","id":"20200331-074452_236970363","dateCreated":"2020-03-31T07:44:52+0000","dateStarted":"2020-03-31T23:25:22+0000","dateFinished":"2020-03-31T23:26:28+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:761"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-03-31T23:25:22+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585697122660_-1091092817","id":"20200331-232522_1449192221","dateCreated":"2020-03-31T23:25:22+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:762"}],"name":"BigCer/Market-Basket-Analysis-Create-function","id":"2F5ZUA5MJ","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}